{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df['sentiment'] = df['sentiment'].map({0:0,4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      "sentiment    1600000 non-null int64\n",
      "text         1596714 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1596714 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      "sentiment    1596714 non-null int64\n",
      "text         1596714 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 36.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text'] #define all other columns except the target variable\n",
    "y = df['sentiment'] #define the target variable\n",
    "\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size = 0.02, random_state = 42)\n",
    "\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, \n",
    "                                                              test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1564779 entries, where 49.99% are positive and 50.01% are negative\n",
      "Validation set has 15967 entries, where 49.82% are positive and 50.18% are negative\n",
      "Testing set has 15968 entries, where 50.33% are positive and 49.67% are negative\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {0} entries, where {1:.2f}% are positive and {2:.2f}% are negative\".\n",
    "      format(len(x_train),len(x_train[y_train==1])/len(x_train)*100, len(x_train[y_train==0])/len(x_train)*100))\n",
    "print(\"Validation set has {0} entries, where {1:.2f}% are positive and {2:.2f}% are negative\".\n",
    "      format(len(x_validation),len(x_validation[y_validation==1])/len(x_validation)*100, \n",
    "             len(x_validation[y_validation==0])/len(x_validation)*100))\n",
    "print(\"Testing set has {0} entries, where {1:.2f}% are positive and {2:.2f}% are negative\".\n",
    "      format(len(x_test),len(x_test[y_test==1])/len(x_test)*100, \n",
    "             len(x_test[y_test==0])/len(x_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Fifth.ipynb](https://github.com/rkritika1508/Sentiment-Analysis/blob/master/Fifth.ipynb), we saw that TfidfVectorizer at 90000 features upto bigram gives the highest validation accuracy at 82.45% using Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tvec = TfidfVectorizer(max_features=90000, ngram_range=(1,2))\n",
    "tvec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tvec.transform(x_train)\n",
    "x_validation_tfidf = tvec.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8378691176198044"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8249514623911818"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_validation_tfidf, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1564779, 90000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1564779"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train_tfidf)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1564779,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(X_data, Y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    no_of_batches = samples_per_epoch/batch_size\n",
    "    counter = 0\n",
    "    index = np.arange(Y_data.shape[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter : batch_size*(counter + 1)]\n",
    "        X_batch = X_data[index_batch, :].toarray()\n",
    "        Y_batch = Y_data[Y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch, Y_batch\n",
    "        if (counter> no_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "48900/48899 [==============================] - 8132s 166ms/step - loss: 0.4090 - acc: 0.8136 - val_loss: 0.3898 - val_acc: 0.8264\n",
      "Epoch 2/5\n",
      "48900/48899 [==============================] - 7927s 162ms/step - loss: 0.3728 - acc: 0.8347 - val_loss: 0.3905 - val_acc: 0.8267\n",
      "Epoch 3/5\n",
      "48900/48899 [==============================] - 9242s 189ms/step - loss: 0.3556 - acc: 0.8450 - val_loss: 0.4034 - val_acc: 0.8224\n",
      "Epoch 4/5\n",
      "48900/48899 [==============================] - 8044s 165ms/step - loss: 0.3305 - acc: 0.8592 - val_loss: 0.4256 - val_acc: 0.8138\n",
      "Epoch 5/5\n",
      "48900/48899 [==============================] - 7371s 151ms/step - loss: 0.3050 - acc: 0.8727 - val_loss: 0.4510 - val_acc: 0.8066\n",
      "Wall time: 11h 18min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim = 90000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit_generator(generator = batch_gen(x_train_tfidf, y_train, 32), epochs = 5, \n",
    "                   validation_data = (x_validation_tfidf, y_validation), steps_per_epoch = x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer().fit(x_train_tfidf)\n",
    "x_train_tfidf_norm = norm.transform(x_train_tfidf)\n",
    "x_validation_tfidf_norm = norm.transform(x_validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_n = Sequential()\n",
    "model_n.add(Dense(64, activation='relu', input_dim = 90000))\n",
    "model_n.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_n.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_n.fit_generator(generator = batch_gen(x_train_tfidf_norm, y_train, 32), epochs = 5, \n",
    "                      validation_data = (x_validation_tfidf_norm, y_validation), \n",
    "                      steps_per_epoch = x_train_tfidf_norm.shape[0]/32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
